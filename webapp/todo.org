* priorities
** DONE crop
** DONE sorting
** DONE fix concept viewing for dynamically created content (maybe use css?)
** STARTED stack view of edit concept [5/6]
- [X] save the original plot
- [X] save the final plot
- [X] the whole group can be marked to be tracked
- [X] track sorting
- [X] intermediate plots can be edited to select
- [ ] upload new concept
** STARTED broden initialize [1/4]
    SCHEDULED: <2018-04-27 Thu>
- [X] download the dataset 
- [ ] understand what the dataset means
  http://localhost:5000/notebooks/broden_dataset.ipynb
  read me in the folder downloaded with images
- [ ] watch their oral videos http://netdissect.csail.mit.edu/ 
- [ ] initialize with dataset

[[http://netdissect.csail.mit.edu/final-network-dissection.pdf][Broden]]: Broadly and Densely Labeled Dataset
This is a unification of many datasets

** TODO show approximation strength [0/2]
    SCHEDULED: <2018-04-26 Wed>
- [-] image view approximation [1/2]
  - [X] async update
  - [ ] approximation in terms of only tracked concept
- [ ] show global approximation strength: % variation explained
** TODO apply to healthcare dataset: e.g., just like the STN
    SCHEDULED: <2018-04-27 Thu>
** TODO Random crop take max
   SCHEDULED: <2018-04-28 Thu>
* cherry on top
*** a button to clear local storage
*** take top out of the backend, just handle it frontend
*** track add and track delete us O(nlogn) for each insert and delete operation
*** edit image and choose tracking should be on the same page
*** tracking could be combined with mixed sorting
    try bring that down (this is inefficient b/c I need to convert to string, but
    this may not cost much since n is relatively small)
    
* gym 

This is a set of tasks that may be useful for future research, for this set of 
tasks, just pick tutorials from 
http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html

The goal should be to use 1 hour per day to understand and refactor code

** TODO pytorch tutorial
   SCHEDULED: <2018-04-26 Thu +1d>
   - State "DONE"       from "REVIEW"     [2018-04-26 Thu 10:13]
   :PROPERTIES:
   :LAST_REPEAT: [2018-04-26 Thu 10:13]
   :END:
   
   Instruction: Each day 1 hour
   1) write what tutorial did I do
   2) write a summary of the tutorial
   3) put the refactored code on github

** TODO paper/book reading
   SCHEDULED: <2018-04-27 Fri +1d>   
   - State "DONE"       from "REVIEW"     [2018-04-26 Thu 14:37]
   - State "DONE"       from "FEEDBACK"   [2018-04-25 Wed 15:30]
   :PROPERTIES:
   :LAST_REPEAT: [2018-04-26 Thu 14:37]
   :END:

   Instruction: Each day 1 hour
   1) write down what topic did I read about
   2) write a summary of the topic
  
*** 2018/4/25
 
- Causality: models, reasoning, and Inference

- Page numbers: 1 - 18

- Summary: 

This part (first part of chapter 1) of the book introduces notion of graphical
model. It introduces the notion of d-separation, that is when are two variables
becomes conditionally independent given another. For example,

1) X<-Z->Y and X->Z->Y has (X ind Y | Z) 
2) X->Z<-Y is blocked as it is, if Z is known then X and Y becomes dependent
AKA explain away effect and selection bias An example is if I choose based on
popluation with Z known, X and Y are likely to be negatively correlated

- Comment:

This part of the book serves as a review of probablistic graphical
models. Nothing surprising so far. I do like its explanation on selection
bias. 

*** 2018/4/26

- Causality: models, reasoning, and Inference

- Page numbers: 19 - chapter 2

- Summary: 
  
  More DAG 

- Comment:

  1. doing is different from observing: observing (X=on) is different from the
     action of turing X on because action removes links in the causal graph.
     Consider, turning sprinkler on means that we cannot infer anything about
     the season, while observing the sprinkler is on implies that the season is
     dry
     
     Also this assumes the graph is a causal graph
     
  2. To be a causual network, one need to ensure that intervention on a set of
     variable won't change the conditional distribution of other variables. 

  3. Infering causal links from observations
     a) temporal clues

* interpretability story
  SCHEDULED: <2018-04-27 Fri +1d>
  - State "DONE"       from "REVIEW"     [2018-04-26 Thu 10:18]
  - State "DONE"       from "FEEDBACK"   [2018-04-25 Wed 15:39]
  :PROPERTIES:
  :LAST_REPEAT: [2018-04-26 Thu 10:18]
  :END:
  
** taxonomy 

  non linear interpretability can be approximately categorized into
  *approximation based method* and *non approximation method*

  I'm more interested in non approximation method because no useful guarantee on
  approximation algorithm exist to date.

  Within *non approximation methods*, we have explaination models (my work), and
  building interpretable model (decision tree, linear sparse
  models). Explanation models are interesting because they don't assume a
  functional form of the model used (explanation doesn't have to guide model's
  optimization).

** motivation
   
   Temporarily, just call my model *interpretable dimension*.

   The key observation is: 
   individual neurons doesn't have to be interpretable

   This observation is backed by several research. Thus, feature visualization
   (or network disection using multiple concepts) may just be a waste of time.

   Instead, we can define dimensions in the feature space that we can understand
   and then project the decision making process on to that space. 

   Can we explain decision rules to Aliens?

** REVIEW use cases
   
   Here I brainstorm several use cases of *interpretable dimension*

   - understand the decision process
     e1: predict network prediction 
     s1: let the user play with the interface for a while, then given a new 
     instance, ask the user to guess the networks' decision
     m1: measure time it took a person to make the decision and the prediction 
     accuracy

   - debugging one's network
     e1: wolf and dog (wolf always with background of snow)
     s1: use snow as background concept and if its weight is very high
     we know fishy stuff are going on
     m1: generate many of such traps, and see how many users can identify

   - correct network behavior (see insights section)
     e1: we know that snow is the confounding variable
     s1: remove snow in dataset, or penalyze the use of that feature
     m1: measure network performance after removal of feature

   - work flow for debugging
     Two view of interpretability: the global view v.s. the local view
     The global view can help users understand what the model is looking for in a 
     specific class, while the local view give per image explanation.
     
     Here's a useful scenario:
     I suspect "wolf" class is biased, so I first look at global view, looking for
     systematic bias in a figure. Hopefully I can find a flaw there. If not, I look
     at pictures where wolf is misclassified and see what the patterns those images
     are picking on, so I know where the confusion is

** REVIEW questions to answer

One assumption I have is that the chosen directions are interpretable, I need
to establish they are. So the question is:
1. Can I use IoU method to show that they are interpretable (each dimension
   corresponds to one and only one concept)? 

The analysis is simple, if they are indeed more interpretable, I got what I
want.  If they are not, I also know that the most uninterpretable dimension is
where the confusion in the network happens (essentially, the network can not
distinguish those concepts, I then found a bug). So it is a win both ways.

Once we know the network has sensible idea of concepts, can a user easily spot 
problem in the network?

use customized concepts
1. Can you predict what the network will output? how fast can you do that
2. Can you identify flaws in this model? how fast can you do that

What if we don't let users mess around with concepts? Can they explain what the
network is doing to a alien? This is just to confirm that the success in
previous question is not caused by people liking the network if they play with
them

use the predefined concepts (can you explain to an Alien)
1. Can you predict what the network will output? how fast can you do that
2. Can you identify flaws in this model? how fast can you do that

While playing with the network's concept is interesting, it can also be
boring. Can attention mechanism help users better understand the network?

use predefined concepts but attended by the attention network
1. Can you predict what the network will output? how fast can you do that
2. Can you identify flaws in this model? how fast can you do that

A case study of two views: to demonstrate how local view complement global view
1. Give a user both views of a misclassified image, see if a user can identify
   the issue? how quickly can they identify the issue
2. Do the same task as 1 but only give global view

** TODO experiments
   
   For each of the question, I should come up with a way to generate data. 
   I think I should start with close world assumption, that is restrict the 
   number of concepts 

** insights and discussion
*** TODO on deduplicate concepts
    Often, the concepts provided contains duplicate features. Can we encourage
    explanation to use as diverse of concepts as possible? 

    There are these stages where features used can be different
    1. in initializing concepts rely on random choosing? maybe just rely on
       maximizing the distance within classes is enough, worry about this once
       problems are encountered
    2. in fine-tuning concepts
       intialize STN randomly? not a good idea
       penalyze already chosen features?

*** on fine-tuning concepts
   Finding concept is tedius. In this work we assume we have a good way of
   finding concept.

   However, given a set of concepts, fine-tuning concept is just a matter of
   finding the attention of the network. It is still not approximation b/c the
   attention can be arbitrary (if you think about it, manully croping an image
   is just finding attention).

   That said, effectively, fintuning an object can be thought of as applying
   transformations to the input image so that its rank is increased. This is an
   optimization problem: we can train an STN to do exactly this task.

   Here are some ideas:
   1. Train STN
      input: concepts
      output: transformed concepts
      objective: make the result output has a very large dot product with the 
      output class to explain
      
      $\max_{\theta} w^T \phi(T_{\theta|x,w} (x))$  

   2. Random Search
      input: concepts
      output: transformed concepts
      objective: try a few random cropping and take the max dot product with the
      output class to explain

      let's just do this for random cropping

   All of these methods hinges on the fact that the original input should be
   distinct enough that the transformer won't just transform every concepts to
   be the same. In other words, we need different features that make a wolf a
   wolf, not the same. In this work, diversity is acheived by the diversity of
   the initial concept space.
   
*** on fine-tuning the network
    Once we identified issues in the network, can we help it learn better? That
    is can human knowledge used to guide network fine tuning.

    Surely we can. The key is to penalyze fishy concept to bring their weights
    down. Here are a couple of ways I can try to do that
    1. using EYE regularizatio 
       disadvantage: has a tuning parameter lambda
    2. initialize final layer weights differently 
       disadvantage: not able to change the optimization surface so that not
       able to correct data error
       advantage: guide the network to a different local minimum
    3. use a network to initialized parameter 
       disadvantage: also doesn't change optimization surface, 
       many more hyper parameters to tune
       advantage: we don't mess around with the model

* others
** DONE report U01 by email to Jenna
   DEADLINE: <2018-04-27 Fri>
